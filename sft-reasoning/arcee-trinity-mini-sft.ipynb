{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to Modal notebooks!\n",
    "\n",
    "Write Python code and collaborate in real time. Your code runs in Modal's\n",
    "**serverless cloud**, and anyone in the same workspace can join.\n",
    "\n",
    "This notebook comes with some common Python libraries installed. Run\n",
    "cells with `Shift+Enter`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "source": [
    "# %% [markdown]\n",
    "# ## Cell 1: Install Dependencies\n",
    "# Run this cell first to install all required packages\n"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "source": [
    "%uv pip install torch datasets accelerate peft trl bitsandbytes\n",
    "%uv pip install flash-attn --no-build-isolation  # Optional: for faster attention\n",
    "%uv pip install --upgrade transformers --no-cache   "
   ],
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[2mUsing Python 3.12.6 environment at: /usr/local\u001b[0m\r\n",
      "\u001b[37m\u280b\u001b[0m \u001b[2mResolving dependencies...                                                     \u001b[0m\r\u001b[2K\u001b[37m\u2819\u001b[0m \u001b[2mResolving dependencies...                                                     \u001b[0m\r\u001b[2K\u001b[37m\u280b\u001b[0m \u001b[2mResolving dependencies...                                                     \u001b[0m\r\u001b[2K\u001b[37m\u2819\u001b[0m \u001b[2mResolving dependencies...                                                     \u001b[0m\r\u001b[2K\u001b[37m\u2819\u001b[0m \u001b[2mtorch==2.8.0+cu129                                                            \u001b[0m\r\u001b[2K\u001b[37m\u2819\u001b[0m \u001b[2mdatasets==4.5.0                                                               \u001b[0m\r\u001b[2K\u001b[37m\u2819\u001b[0m \u001b[2maccelerate==1.10.1                                                            \u001b[0m\r\u001b[2K\u001b[37m\u2819\u001b[0m \u001b[2mpeft==0.18.1                                                                  \u001b[0m\r\u001b[2K\u001b[37m\u2819\u001b[0m \u001b[2mtrl==0.27.2                                                                   \u001b[0m\r\u001b[2K\u001b[37m\u2819\u001b[0m \u001b[2mbitsandbytes==0.49.1                                                          \u001b[0m\r\u001b[2K\u001b[37m\u2819\u001b[0m \u001b[2mfilelock==3.20.3                                                              \u001b[0m\r\u001b[2K\u001b[37m\u2819\u001b[0m \u001b[2mtyping-extensions==4.15.0                                                     \u001b[0m\r\u001b[2K\u001b[37m\u2819\u001b[0m \u001b[2msetuptools==70.2.0                                                            \u001b[0m\r\u001b[2K\u001b[37m\u2819\u001b[0m \u001b[2msetuptools==70.2.0                                                            \u001b[0m\r\u001b[2K\u001b[37m\u2819\u001b[0m \u001b[2msympy==1.13.3                                                                 \u001b[0m\r\u001b[2K\u001b[37m\u2819\u001b[0m \u001b[2mnetworkx==3.3                                                                 \u001b[0m\r\u001b[2K\u001b[37m\u2819\u001b[0m \u001b[2mjinja2==3.1.4                                                                 \u001b[0m\r\u001b[2K\u001b[37m\u2819\u001b[0m \u001b[2mfsspec==2025.10.0                                                             \u001b[0m\r\u001b[2K\u001b[37m\u2819\u001b[0m \u001b[2mfsspec==2025.10.0                                                             \u001b[0m\r\u001b[2K\u001b[37m\u2819\u001b[0m \u001b[2mnvidia-cuda-nvrtc-cu12==12.9.86                                               \u001b[0m\r\u001b[2K\u001b[37m\u2819\u001b[0m \u001b[2mnvidia-cuda-nvrtc-cu12==12.9.86                                               \u001b[0m\r\u001b[2K\u001b[37m\u2819\u001b[0m \u001b[2mnvidia-cuda-runtime-cu12==12.9.79                                             \u001b[0m\r\u001b[2K\u001b[37m\u2819\u001b[0m \u001b[2mnvidia-cuda-runtime-cu12==12.9.79                                             \u001b[0m\r\u001b[2K\u001b[37m\u2819\u001b[0m \u001b[2maiohttp==3.10.8                                                               \u001b[0m\r\u001b[2K\u001b[37m\u2819\u001b[0m \u001b[2mmultidict==6.1.0                                                              \u001b[0m\r\u001b[2K\u001b[2mResolved \u001b[1m69 packages\u001b[0m \u001b[2min 156ms\u001b[0m\u001b[0m\r\n",
      "\u001b[2mUninstalled \u001b[1m1 package\u001b[0m \u001b[2min 2ms\u001b[0m\u001b[0m\r\n",
      "\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591 [0/0] \u001b[2mInstalling wheels...                                 \u001b[0m\r\u001b[2K\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591 [0/1] \u001b[2mInstalling wheels...                                 \u001b[0m\r\u001b[2K\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591 [0/1] \u001b[2mfsspec==2025.10.0                                    \u001b[0m\r\u001b[2K\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 [1/1] \u001b[2mfsspec==2025.10.0                                    \u001b[0m\r\u001b[2K\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 92ms\u001b[0m\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mfsspec\u001b[0m\u001b[2m==2026.1.0\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mfsspec\u001b[0m\u001b[2m==2025.10.0\u001b[0m\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\u001b[2mUsing Python 3.12.6 environment at: /usr/local\u001b[0m\r\n",
      "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 5ms\u001b[0m\u001b[0m\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\u001b[2mUsing Python 3.12.6 environment at: /usr/local\u001b[0m\r\n",
      "\u001b[37m\u280b\u001b[0m \u001b[2mResolving dependencies...                                                     \u001b[0m\r\u001b[2K\u001b[37m\u280b\u001b[0m \u001b[2mResolving dependencies...                                                     \u001b[0m\r\u001b[2K\u001b[37m\u2819\u001b[0m \u001b[2mResolving dependencies...                                                     \u001b[0m\r\u001b[2K\u001b[37m\u2819\u001b[0m \u001b[2mtransformers==5.0.0                                                           \u001b[0m\r\u001b[2K\u001b[37m\u2819\u001b[0m \u001b[2mfilelock==3.20.3                                                              \u001b[0m\r\u001b[2K\u001b[37m\u2819\u001b[0m \u001b[2mhuggingface-hub==1.3.7                                                        \u001b[0m\r\u001b[2K\u001b[37m\u2819\u001b[0m \u001b[2mnumpy==2.4.2                                                                  \u001b[0m\r\u001b[2K\u001b[37m\u2819\u001b[0m \u001b[2mpackaging==26.0                                                               \u001b[0m\r\u001b[2K\u001b[37m\u2819\u001b[0m \u001b[2mpyyaml==6.0.3                                                                 \u001b[0m\r\u001b[2K\u001b[37m\u2819\u001b[0m \u001b[2mregex==2026.1.15                                                              \u001b[0m\r\u001b[2K\u001b[37m\u2819\u001b[0m \u001b[2mtokenizers==0.22.2                                                            \u001b[0m\r\u001b[2K\u001b[37m\u2819\u001b[0m \u001b[2mtyper-slim==0.21.1                                                            \u001b[0m\r\u001b[2K\u001b[37m\u2819\u001b[0m \u001b[2msafetensors==0.7.0                                                            \u001b[0m\r\u001b[2K\u001b[37m\u2819\u001b[0m \u001b[2mtqdm==4.67.3                                                                  \u001b[0m\r\u001b[2K\u001b[37m\u2819\u001b[0m \u001b[2mfsspec==2026.1.0                                                              \u001b[0m\r\u001b[2K\u001b[37m\u2819\u001b[0m \u001b[2mhf-xet==1.2.0                                                                 \u001b[0m\r\u001b[2K\u001b[37m\u2819\u001b[0m \u001b[2mhf-xet==1.2.0                                                                 \u001b[0m\r\u001b[2K\u001b[37m\u2819\u001b[0m \u001b[2mhttpx==0.28.1                                                                 \u001b[0m\r\u001b[2K\u001b[37m\u2819\u001b[0m \u001b[2mshellingham==1.5.4                                                            \u001b[0m\r\u001b[2K\u001b[37m\u2819\u001b[0m \u001b[2mtyping-extensions==4.15.0                                                     \u001b[0m\r\u001b[2K\u001b[37m\u2819\u001b[0m \u001b[2mclick==8.3.1                                                                  \u001b[0m\r\u001b[2K\u001b[37m\u2819\u001b[0m \u001b[2manyio==4.12.1                                                                 \u001b[0m\r\u001b[2K\u001b[37m\u2819\u001b[0m \u001b[2mtyping-extensions==4.15.0                                                     \u001b[0m\r\u001b[2K\u001b[37m\u2819\u001b[0m \u001b[2mcertifi==2026.1.4                                                             \u001b[0m\r\u001b[2K\u001b[2mResolved \u001b[1m22 packages\u001b[0m \u001b[2min 171ms\u001b[0m\u001b[0m\r\n",
      "\u001b[37m\u280b\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/0)                                                   \r\u001b[2K\u001b[37m\u280b\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)                                                   \r\u001b[2K\u001b[37m\u2819\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)                                                   \r\u001b[2K\u001b[37m\u2819\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)\r\n",
      "\u001b[2mfsspec    \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/197.11 KiB                    \u001b[1A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1A\u001b[37m\u2819\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)\r\n",
      "\u001b[2mfsspec    \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 16.00 KiB/197.11 KiB                  \u001b[1A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1A\u001b[37m\u2819\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)\r\n",
      "\u001b[2mfsspec    \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 32.00 KiB/197.11 KiB                  \u001b[1A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1A\u001b[37m\u2819\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)\r\n",
      "\u001b[2mfsspec    \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 48.00 KiB/197.11 KiB                  \u001b[1A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1A\u001b[37m\u2819\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)\r\n",
      "\u001b[2mfsspec    \u001b[0m \u001b[32m----------\u001b[2m--------------------\u001b[0m\u001b[0m 62.98 KiB/197.11 KiB                  \u001b[1A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1A\u001b[37m\u2819\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)\r\n",
      "\u001b[2mfsspec    \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 78.98 KiB/197.11 KiB                  \u001b[1A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1A\u001b[37m\u2819\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)\r\n",
      "\u001b[2mfsspec    \u001b[0m \u001b[32m---------------\u001b[2m---------------\u001b[0m\u001b[0m 94.98 KiB/197.11 KiB                  \u001b[1A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1A\u001b[37m\u2819\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)\r\n",
      "\u001b[2mfsspec    \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 110.98 KiB/197.11 KiB                 \u001b[1A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1A\u001b[37m\u2819\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)\r\n",
      "\u001b[2mfsspec    \u001b[0m \u001b[32m--------------------\u001b[2m----------\u001b[0m\u001b[0m 126.98 KiB/197.11 KiB                 \u001b[1A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1A\u001b[37m\u2819\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)\r\n",
      "\u001b[2mfsspec    \u001b[0m \u001b[32m----------------------\u001b[2m--------\u001b[0m\u001b[0m 142.98 KiB/197.11 KiB                 \u001b[1A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1A\u001b[37m\u2819\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)\r\n",
      "\u001b[2mfsspec    \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 158.98 KiB/197.11 KiB                 \u001b[1A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1A\u001b[37m\u2819\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)\r\n",
      "\u001b[2mfsspec    \u001b[0m \u001b[32m---------------------------\u001b[2m---\u001b[0m\u001b[0m 174.98 KiB/197.11 KiB                 \u001b[1A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1A\u001b[37m\u2819\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)\r\n",
      "\u001b[2mfsspec    \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 190.98 KiB/197.11 KiB                 \u001b[1A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1A\u001b[37m\u2819\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)\r\n",
      "\u001b[2mfsspec    \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 197.11 KiB/197.11 KiB                 \u001b[1A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1A\u001b[37m\u2819\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)                                                   \r\u001b[2K\u001b[37m\u2819\u001b[0m \u001b[2m\u001b[0m (1/1)                                                                        \r\u001b[2K\u001b[2mPrepared \u001b[1m1 package\u001b[0m \u001b[2min 13ms\u001b[0m\u001b[0m\r\n",
      "\u001b[2mUninstalled \u001b[1m1 package\u001b[0m \u001b[2min 1ms\u001b[0m\u001b[0m\r\n",
      "\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591 [0/0] \u001b[2mInstalling wheels...                                 \u001b[0m\r\u001b[2K\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591 [0/1] \u001b[2mInstalling wheels...                                 \u001b[0m\r\u001b[2K\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591 [0/1] \u001b[2mfsspec==2026.1.0                                     \u001b[0m\r\u001b[2K\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 [1/1] \u001b[2mfsspec==2026.1.0                                     \u001b[0m\r\u001b[2K\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 84ms\u001b[0m\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mfsspec\u001b[0m\u001b[2m==2025.10.0\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mfsspec\u001b[0m\u001b[2m==2026.1.0\u001b[0m\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "source": [
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Optional, List, Dict, Any\n",
    "\n",
    "import torch\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer, SFTConfig"
   ],
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "source": [
    "MODEL_NAME = \"arcee-ai/Trinity-Mini\"\n",
    "TRACES_FILE = \"reasoning_traces_correct.json\"  # Path to filtered correct traces\n",
    "OUTPUT_DIR = \"/mnt/models/trinity-triton-sft\"\n",
    "MAX_LENGTH = 4096  "
   ],
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "source": [
    "# Training hyperparameters\n",
    "LEARNING_RATE = 2e-4\n",
    "BATCH_SIZE = 1\n",
    "GRADIENT_ACCUMULATION_STEPS = 8  # Effective batch size = 1 * 8 = 8\n",
    "NUM_EPOCHS = 3\n",
    "WARMUP_RATIO = 0.1\n",
    "\n",
    "# LoRA configuration for efficient finetuning\n",
    "LORA_R = 64\n",
    "LORA_ALPHA = 128\n",
    "LORA_DROPOUT = 0.05"
   ],
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "source": [
    "def load_and_filter_traces(\n",
    "    traces_file: str,\n",
    "    require_correctness: bool = False,  # Default: include all (correct + failed)\n",
    "    require_fast_0: bool = False,  # Default: include all\n",
    "    require_fast_1: bool = False,  # Default: include all\n",
    "    require_fast_2: bool = False,  # Default: include all\n",
    "    min_speedup: Optional[float] = None,\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Load reasoning traces from JSON and optionally filter based on quality criteria.\n",
    "\n",
    "    DEFAULT BEHAVIOR: Includes ALL traces (correct + failed) from the file.\n",
    "    The reasoning_traces_correct.json is already curated with the desired mix.\n",
    "\n",
    "    Training on both correct and failed traces helps the model:\n",
    "    - Learn successful Triton patterns from correct examples\n",
    "    - Understand common pitfalls and errors from failed examples\n",
    "    - Develop better reasoning about what works and what doesn't\n",
    "\n",
    "    Args:\n",
    "        traces_file: Path to the reasoning_traces_correct.json file\n",
    "        require_correctness: If True, only include correct traces\n",
    "        require_fast_0: If True, only include traces where kernel compiled correctly\n",
    "        require_fast_1: If True, only include traces with speedup > 1.0\n",
    "        require_fast_2: If True, only include traces with speedup >= 2.0\n",
    "        min_speedup: Optional minimum speedup threshold\n",
    "\n",
    "    Returns:\n",
    "        List of filtered trace dictionaries\n",
    "    \"\"\"\n",
    "    # Load traces\n",
    "    traces_path = Path(traces_file)\n",
    "    if not traces_path.exists():\n",
    "        raise FileNotFoundError(\n",
    "            f\"Traces file not found at {traces_path}. \"\n",
    "            \"Please run the orchestrator first to generate traces: \"\n",
    "            \"python orchestrator.py\"\n",
    "        )\n",
    "    \n",
    "    with open(traces_path, \"r\") as f:\n",
    "        traces = json.load(f)\n",
    "    \n",
    "    print(f\"Loaded {len(traces)} total traces\")\n",
    "    \n",
    "    # Count correct vs failed for statistics\n",
    "    correct_count = sum(1 for t in traces if t.get(\"result\", {}).get(\"correctness\", False))\n",
    "    failed_count = len(traces) - correct_count\n",
    "\n",
    "    # Apply optional filters\n",
    "    filtered_traces = []\n",
    "    filtered_out = 0\n",
    "\n",
    "    for trace in traces:\n",
    "        result = trace.get(\"result\", {})\n",
    "\n",
    "        # Apply filters only if explicitly requested\n",
    "        if require_correctness and not result.get(\"correctness\", False):\n",
    "            filtered_out += 1\n",
    "            continue\n",
    "\n",
    "        if require_fast_0 and not result.get(\"fast_0\", False):\n",
    "            filtered_out += 1\n",
    "            continue\n",
    "\n",
    "        if require_fast_1 and not result.get(\"fast_1\", False):\n",
    "            filtered_out += 1\n",
    "            continue\n",
    "\n",
    "        if require_fast_2 and not result.get(\"fast_2\", False):\n",
    "            filtered_out += 1\n",
    "            continue\n",
    "\n",
    "        if min_speedup is not None:\n",
    "            speedup = result.get(\"speedup\", 0.0)\n",
    "            if speedup < min_speedup:\n",
    "                filtered_out += 1\n",
    "                continue\n",
    "\n",
    "        # Trace passed all filters\n",
    "        filtered_traces.append(trace)\n",
    "\n",
    "    stats = {\n",
    "        \"total\": len(traces),\n",
    "        \"correct\": correct_count,\n",
    "        \"failed\": failed_count,\n",
    "        \"filtered_out\": filtered_out,\n",
    "        \"final_count\": len(filtered_traces),\n",
    "    }\n",
    "    \n",
    "    # Print statistics\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"DATASET LOADING SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Total traces loaded:       {stats['total']}\")\n",
    "    print(f\"  \u2713 Correct kernels:       {stats['correct']} ({stats['correct']/stats['total']*100:.1f}%)\")\n",
    "    print(f\"  \u2717 Failed kernels:        {stats['failed']} ({stats['failed']/stats['total']*100:.1f}%)\")\n",
    "\n",
    "    if stats['filtered_out'] > 0:\n",
    "        print(f\"\\nFiltered out:              {stats['filtered_out']} traces\")\n",
    "\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"Final training set:        {stats['final_count']} traces\")\n",
    "\n",
    "    if stats['final_count'] > 0:\n",
    "        final_correct = sum(1 for t in filtered_traces if t.get(\"result\", {}).get(\"correctness\", False))\n",
    "        final_failed = stats['final_count'] - final_correct\n",
    "        print(f\"  Training mix:            {final_correct/stats['final_count']*100:.1f}% correct, \"\n",
    "              f\"{final_failed/stats['final_count']*100:.1f}% failed\")\n",
    "\n",
    "    print(\"=\" * 60 + \"\\n\")\n",
    "    \n",
    "    return filtered_traces"
   ],
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "source": [
    "try:\n",
    "    traces = load_and_filter_traces(TRACES_FILE)  # Include ALL traces by default\n",
    "    print(f\"\u2713 Successfully loaded {len(traces)} traces for training\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Note: {e}\")\n",
    "    traces = []"
   ],
   "execution_count": 7,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loaded 158 total traces\n",
      "\n",
      "============================================================\n",
      "DATASET LOADING SUMMARY\n",
      "============================================================\n",
      "Total traces loaded:       158\n",
      "  \u2713 Correct kernels:       133 (84.2%)\n",
      "  \u2717 Failed kernels:        25 (15.8%)\n",
      "------------------------------------------------------------\n",
      "Final training set:        158 traces\n",
      "  Training mix:            84.2% correct, 15.8% failed\n",
      "============================================================\n",
      "\n",
      "\u2713 Successfully loaded 158 traces for training\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "source": [
    "def format_trace_for_sft(trace: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Format a single trace into the SFT conversational format.\n",
    "\n",
    "    The format follows the pattern:\n",
    "    User: PyTorch code\n",
    "    Assistant: <think>reasoning</think><triton>code</triton>\n",
    "    \"\"\"\n",
    "    pytorch_code = trace.get(\"pytorch_code\", \"\")\n",
    "    # Prefer model_reasoning (actual reasoning process), fallback to thinking (polished explanation)\n",
    "    reasoning = trace.get(\"model_reasoning\") or trace.get(\"thinking\", \"\")\n",
    "    triton_code = trace.get(\"triton_code\", \"\")\n",
    "\n",
    "    # Build the user message\n",
    "    user_content = f\"\"\"Convert the following PyTorch code to an optimized Triton kernel:\n",
    "\n",
    "```python\n",
    "{pytorch_code}\n",
    "```\n",
    "\n",
    "Generate a complete Triton implementation that produces the same output as the PyTorch code.\"\"\"\n",
    "\n",
    "    # Build the assistant message with reasoning and code\n",
    "    # Always wrap reasoning in <think></think> tags\n",
    "    if reasoning:\n",
    "        if not reasoning.strip().startswith(\"<think>\"):\n",
    "            reasoning = f\"<think>\\n{reasoning}\\n</think>\"\n",
    "    else:\n",
    "        reasoning = \"<think>\\n</think>\"\n",
    "\n",
    "    # Wrap triton code in <triton></triton> tags\n",
    "    if triton_code:\n",
    "        if not triton_code.strip().startswith(\"<triton>\"):\n",
    "            triton_code = f\"<triton>\\n{triton_code}\\n</triton>\"\n",
    "    else:\n",
    "        triton_code = \"<triton>\\n</triton>\"\n",
    "\n",
    "    assistant_content = f\"{reasoning}\\n\\n{triton_code}\"\n",
    "\n",
    "    # Return in conversational format for SFTTrainer\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": user_content},\n",
    "            {\"role\": \"assistant\", \"content\": assistant_content}\n",
    "        ]\n",
    "    }\n"
   ],
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "source": [
    "def prepare_dataset(traces: List[Dict[str, Any]], test_size: float = 0.1) -> DatasetDict:\n",
    "    \"\"\"\n",
    "    Prepare a HuggingFace DatasetDict from traces.\n",
    "\n",
    "    Args:\n",
    "        traces: List of trace dictionaries\n",
    "        test_size: Fraction of data to use for validation\n",
    "\n",
    "    Returns:\n",
    "        DatasetDict with 'train' and 'test' splits\n",
    "    \"\"\"\n",
    "    if not traces:\n",
    "        raise ValueError(\"No traces provided! Cannot create dataset.\")\n",
    "\n",
    "    # Format all traces for SFT\n",
    "    formatted_data = [format_trace_for_sft(trace) for trace in traces]\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = Dataset.from_list(formatted_data)\n",
    "\n",
    "    # Split into train/test\n",
    "    if len(dataset) > 1:\n",
    "        split = dataset.train_test_split(test_size=test_size, seed=42)\n",
    "        return split\n",
    "    else:\n",
    "        # Not enough data for split, use same for both\n",
    "        return DatasetDict({\n",
    "            \"train\": dataset,\n",
    "            \"test\": dataset\n",
    "        })\n",
    "\n",
    "\n",
    "# Prepare the dataset\n",
    "dataset = prepare_dataset(traces)\n",
    "print(f\"\\nDataset prepared:\")\n",
    "print(f\"  Train samples: {len(dataset['train'])}\")\n",
    "print(f\"  Test samples:  {len(dataset['test'])}\")\n",
    "\n",
    "# Preview a sample\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SAMPLE TRAINING EXAMPLE\")\n",
    "print(\"=\" * 60)\n",
    "if len(dataset['train']) > 0:\n",
    "    sample = dataset['train'][-1]\n",
    "    print(f\"User message (truncated):\\n{sample['messages'][0]['content']}...\")\n",
    "    print(f\"\\nAssistant message (truncated):\\n{sample['messages'][1]['content']}...\")\n"
   ],
   "execution_count": 9,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Dataset prepared:\n",
      "  Train samples: 142\n",
      "  Test samples:  16\n",
      "\n",
      "============================================================\n",
      "SAMPLE TRAINING EXAMPLE\n",
      "============================================================\n",
      "User message (truncated):\n",
      "Convert the following PyTorch code to an optimized Triton kernel:\n",
      "\n",
      "```python\n",
      "import math\n",
      "import torch\n",
      "from torch import nn\n",
      "from torch.nn import functional as F\n",
      "import torch.utils.data\n",
      "\n",
      "\n",
      "def matmul(x, y):\n",
      "    if x.dim() == y.dim():\n",
      "        return x @ y\n",
      "    if x.dim() == y.dim() - 1:\n",
      "        return (x.unsqueeze(-2) @ y).squeeze(-2)\n",
      "    return (x @ y.unsqueeze(-2)).squeeze(-2)\n",
      "\n",
      "\n",
      "class LayerNorm(nn.Module):\n",
      "\n",
      "    def __init__(self, d_model, eps=1e-06):\n",
      "        super().__init__()\n",
      "        self.gamma = nn.Parameter(torch.ones(d_model))\n",
      "        self.beta = nn.Parameter(torch.zeros(d_model))\n",
      "        self.eps = eps\n",
      "\n",
      "    def forward(self, x):\n",
      "        mean = x.mean(-1, keepdim=True)\n",
      "        std = x.std(-1, keepdim=True)\n",
      "        return self.gamma * (x - mean) / (std + self.eps) + self.beta\n",
      "\n",
      "\n",
      "class ResidualBlock(nn.Module):\n",
      "\n",
      "    def __init__(self, layer, d_model, dropout_ratio):\n",
      "        super().__init__()\n",
      "        self.layer = layer\n",
      "        self.dropout = nn.Dropout(dropout_ratio)\n",
      "        self.layernorm = LayerNorm(d_model)\n",
      "\n",
      "    def forward(self, *x, padding=None):\n",
      "        return self.layernorm(x[0] + self.dropout(self.layer(*x, padding=\n",
      "            padding)))\n",
      "\n",
      "\n",
      "class Attention(nn.Module):\n",
      "\n",
      "    def __init__(self, d_key, dropout_ratio, causal):\n",
      "        super().__init__()\n",
      "        self.scale = math.sqrt(d_key)\n",
      "        self.dropout = nn.Dropout(dropout_ratio)\n",
      "        self.causal = causal\n",
      "\n",
      "    def forward(self, query, key, value, padding=None):\n",
      "        dot_products = matmul(query, key.transpose(1, 2))\n",
      "        if query.dim() == 3 and self.causal:\n",
      "            tri = key.new_ones((key.size(1), key.size(1))).triu(1) * INF\n",
      "            dot_products.sub_(tri.unsqueeze(0))\n",
      "        if padding is not None:\n",
      "            if dot_products.dim() == 3:\n",
      "                dot_products.masked_fill_(padding.unsqueeze(1).expand_as(\n",
      "                    dot_products), -INF)\n",
      "            else:\n",
      "                dot_products.masked_fill_(padding, -INF)\n",
      "        return matmul(self.dropout(F.softmax(dot_products / self.scale, dim\n",
      "            =-1)), value)\n",
      "\n",
      "\n",
      "class Linear(nn.Linear):\n",
      "\n",
      "    def forward(self, x):\n",
      "        size = x.size()\n",
      "        return super().forward(x.contiguous().view(-1, size[-1])).view(*\n",
      "            size[:-1], -1)\n",
      "\n",
      "\n",
      "class MultiHead(nn.Module):\n",
      "\n",
      "    def __init__(self, d_key, d_value, n_heads, dropout_ratio, causal=False):\n",
      "        super().__init__()\n",
      "        self.attention = Attention(d_key, dropout_ratio, causal=causal)\n",
      "        self.wq = Linear(d_key, d_key, bias=False)\n",
      "        self.wk = Linear(d_key, d_key, bias=False)\n",
      "        self.wv = Linear(d_value, d_value, bias=False)\n",
      "        self.n_heads = n_heads\n",
      "\n",
      "    def forward(self, query, key, value, padding=None):\n",
      "        query, key, value = self.wq(query), self.wk(key), self.wv(value)\n",
      "        query, key, value = (x.chunk(self.n_heads, -1) for x in (query, key,\n",
      "            value))\n",
      "        return torch.cat([self.attention(q, k, v, padding=padding) for q, k,\n",
      "            v in zip(query, key, value)], -1)\n",
      "\n",
      "\n",
      "class Feedforward(nn.Module):\n",
      "\n",
      "    def __init__(self, d_in, d_out, activation=None, bias=True, dropout=0.2):\n",
      "        super().__init__()\n",
      "        if activation is not None:\n",
      "            self.activation = getattr(torch, activation)\n",
      "        else:\n",
      "            self.activation = lambda x: x\n",
      "        self.linear = Linear(d_in, d_out, bias=bias)\n",
      "        self.dropout = nn.Dropout(dropout)\n",
      "\n",
      "    def forward(self, x):\n",
      "        return self.activation(self.linear(self.dropout(x)))\n",
      "\n",
      "\n",
      "class LinearReLU(nn.Module):\n",
      "\n",
      "    def __init__(self, d_model, d_hidden):\n",
      "        super().__init__()\n",
      "        self.feedforward = Feedforward(d_model, d_hidden, activation='relu')\n",
      "        self.linear = Linear(d_hidden, d_model)\n",
      "\n",
      "    def forward(self, x, padding=None):\n",
      "        return self.linear(self.feedforward(x))\n",
      "\n",
      "\n",
      "class TransformerEncoderLayer(nn.Module):\n",
      "\n",
      "    def __init__(self, dimension, n_heads, hidden, dropout):\n",
      "        super().__init__()\n",
      "        self.selfattn = ResidualBlock(MultiHead(dimension, dimension,\n",
      "            n_heads, dropout), dimension, dropout)\n",
      "        self.feedforward = ResidualBlock(LinearReLU(dimension, hidden),\n",
      "            dimension, dropout)\n",
      "\n",
      "    def forward(self, x, padding=None):\n",
      "        return self.feedforward(self.selfattn(x, x, x, padding=padding))\n",
      "\n",
      "\n",
      "def get_inputs():\n",
      "    return [torch.rand([4, 4, 4])]\n",
      "\n",
      "\n",
      "def get_init_inputs():\n",
      "    return [[], {'dimension': 4, 'n_heads': 4, 'hidden': 4, 'dropout': 0.5}]\n",
      "\n",
      "```\n",
      "\n",
      "Generate a complete Triton implementation that produces the same output as the PyTorch code....\n",
      "\n",
      "Assistant message (truncated):\n",
      "<think>\n",
      "We need to convert the whole model to Triton kernel? That's huge. The task: \"Convert the following PyTorch code to an optimized Triton kernel\". Likely they want a kernel that replicates the forward pass of TransformerEncoderLayer given inputs and parameters (weights random). We need wrapper function with same inputs as Model.forward() i.e., x and optional padding. The wrapper should create random weights for all Linear layers etc. Or accept them as extra parameters. Simpler: generate random weights inside wrapper.\n",
      "\n",
      "But Triton kernels are for low-level ops; implementing whole transformer with attention, layernorm, etc. Might be large but we can implement each operation as separate kernels: layernorm, linear, softmax, matmul, etc., and compose them in wrapper using torch calls to launch kernels. However the requirement: \"Generate a complete Triton implementation that produces the same output as the PyTorch code.\" Could implement a single kernel that does all steps for given dimensions (dimension, n_heads, hidden). Since dimensions are small (4), we could just implement naive elementwise operations using Triton.\n",
      "\n",
      "Simplify: Use Triton to implement matmul, layernorm, softmax with masking, dropout (skip dropout or set dropout=0). Since dropout randomness may differ but we can set dropout ratio to 0 for deterministic output? The original dropout ratio is 0.5. To match output, we need same random dropout mask as PyTorch? That's impossible without replicating torch's RNG. We could ignore dropout (set to identity). Probably acceptable for demonstration.\n",
      "\n",
      "Thus we will implement core operations: matmul kernel, layernorm kernel, softmax kernel, attention kernel (including scaling and optional causal mask). Use wrapper to instantiate random weights for Linear layers (wq, wk, wv, etc.) and biases (none). We'll implement Linear as matmul with weight (no bias). Use Triton kernels to compute.\n",
      "\n",
      "We'll assume input shape (B, S, D) = [4,4,4]. n_heads=4, so each head dimension = D / n_heads = 1.\n",
      "\n",
      "Implement kernels:\n",
      "\n",
      "- matmul_kernel: compute C = A @ B where A shape (M,K), B shape (K,N). Use block sizes BLOCK_M, BLOCK_N, BLOCK_K.\n",
      "\n",
      "- layernorm_kernel: compute per-last-dim mean and variance, then normalize.\n",
      "\n",
      "- softmax_kernel: compute softmax over last dim.\n",
      "\n",
      "- attention_kernel: combine matmul QK^T, apply scaling, optional causal mask, softmax, then matmul with V.\n",
      "\n",
      "Given small dims, we can use simple elementwise kernels.\n",
      "\n",
      "Implement wrapper:\n",
      "\n",
      "def triton_kernel_wrapper(x, padding=None, dimension=4, n_heads=4, hidden=4, dropout=0.5, causal=False):\n",
      "\n",
      "Create weight tensors:\n",
      "\n",
      "wq = torch.randn(dimension, dimension, device=x.device, dtype=x.dtype)\n",
      "wk = torch.randn(dimension, dimension, device=x.device)\n",
      "wv = torch.randn(dimension, dimension, device=x.device)\n",
      "wo = torch.randn(dimension, dimension, device=x.device) (final linear after multihead concat)\n",
      "\n",
      "linear1 weight: w1 = torch.randn(dimension, hidden)\n",
      "linear2 weight: w2 = torch.randn(hidden, dimension)\n",
      "\n",
      "layernorm gamma and beta: gamma = torch.ones(dimension), beta = torch.zeros(dimension)\n",
      "\n",
      "Implement forward using kernels:\n",
      "\n",
      "- Linear: matmul(x, w) (x shape B,S,D ; w shape D, D_out). Use matmul kernel.\n",
      "\n",
      "- Split heads: reshape to (B, S, n_heads, head_dim) then transpose to (B, n_heads, S, head_dim). Since head_dim=1, we can just view.\n",
      "\n",
      "- Compute attention per head: using matmul kernel for QK^T: Q shape (B, n_heads, S, hd) ; K shape (B, n_heads, S, hd) -> need transpose last two dims of K: K_T shape (B, n_heads, hd, S). matmul yields (B, n_heads, S, S). Apply scaling, causal mask (triangular INF). Use softmax kernel per (B, n_heads, S) row.\n",
      "\n",
      "- Multiply with V: V shape (B, n_heads, S, hd). Result shape (B, n_heads, S, hd). Then concat heads: reshape to (B,S,D).\n",
      "\n",
      "- Apply residual + layernorm: residual connection after dropout (skip). Use layernorm kernel on (x + attn_out).\n",
      "\n",
      "- Feedforward: dropout (skip) then linear1 (x * w1) -> activation ReLU -> linear2 (w2) -> residual + layernorm.\n",
      "\n",
      "We'll implement ReLU with tl.maximum.\n",
      "\n",
      "We'll need INF constant: a large number like 1e9.\n",
      "\n",
      "Implement kernels:\n",
      "\n",
      "1. matmul_kernel(A_ptr, B_ptr, C_ptr, M, N, K, stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr)\n",
      "\n",
      "Standard Triton GEMM.\n",
      "\n",
      "2. layernorm_kernel(x_ptr, gamma_ptr, beta_ptr, out_ptr, N, epsilon, stride_xn, stride_xc, stride_outn, stride_outc) where N is feature dim.\n",
      "\n",
      "We'll compute per row (batch*seq) mean and var.\n",
      "\n",
      "Simplify: use block size = 128 and compute per row.\n",
      "\n",
      "Given small dims, we can just load whole row.\n",
      "\n",
      "3. softmax_kernel(x_ptr, out_ptr, rows, cols, stride_xc, stride_xr, stride_outc, stride_outr, BLOCK: tl.constexpr)\n",
      "\n",
      "Compute max per row, subtract, exp, sum, divide.\n",
      "\n",
      "4. dropout kernel: skip.\n",
      "\n",
      "Now produce wrapper.\n",
      "\n",
      "Need grid calculations.\n",
      "\n",
      "We'll use triton.cdiv for block sizes.\n",
      "\n",
      "Now write answer in required format.\n",
      "\n",
      "\n",
      "</think>\n",
      "\n",
      "<triton>\n",
      "import torch\n",
      "import triton\n",
      "import triton.language as tl\n",
      "\n",
      "INF = 1e9  # large value used for masking\n",
      "\n",
      "\n",
      "# --------------------------------------------------------------\n",
      "# 1. Tiled GEMM (batched)\n",
      "# --------------------------------------------------------------\n",
      "@triton.jit\n",
      "def gemm_kernel(\n",
      "    A, B, C,\n",
      "    stride_am, stride_ak,          # A: (M, K)\n",
      "    stride_bk, stride_bn,          # B: (K, N)\n",
      "    stride_cm, stride_cn,          # C: (M, N)\n",
      "    M, N, K,\n",
      "    BLOCK_M: tl.constexpr,\n",
      "    BLOCK_N: tl.constexpr,\n",
      "    BLOCK_K: tl.constexpr,\n",
      "    num_warps: tl.constexpr\n",
      "):\n",
      "    pid_m = tl.program_id(0)\n",
      "    pid_n = tl.program_id(1)\n",
      "\n",
      "    # compute block start offsets\n",
      "    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n",
      "    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n",
      "\n",
      "    # mask for out\u2011of\u2011bounds\n",
      "    mask_m = offs_m < M\n",
      "    mask_n = offs_n < N\n",
      "\n",
      "    # pointers to the start of the block\n",
      "    a_ptrs = A + (offs_m[:, None] * stride_am + tl.arange(0, BLOCK_K)[None, :] * stride_ak)\n",
      "    b_ptrs = B + (tl.arange(0, BLOCK_K)[:, None] * stride_bk + offs_n[None, :] * stride_bn)\n",
      "\n",
      "    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n",
      "\n",
      "    # loop over K dimension\n",
      "    for k in range(0, K, BLOCK_K):\n",
      "        cur_k = min(BLOCK_K, K - k)\n",
      "\n",
      "        a = tl.load(a_ptrs + k * stride_ak, mask=mask_m[:, None] & (tl.arange(0, cur_k)[None, :] < cur_k), other=0.0)\n",
      "        b = tl.load(b_ptrs + k * stride_bk, mask=(tl.arange(0, cur_k)[:, None] < cur_k) & mask_n[None, :], other=0.0)\n",
      "\n",
      "        acc += tl.dot(a, b)\n",
      "\n",
      "    # write back\n",
      "    c_ptrs = C + (offs_m[:, None] * stride_cm + offs_n[None, :] * stride_cn)\n",
      "    tl.store(c_ptrs, acc, mask=mask_m[:, None] & mask_n[None, :])\n",
      "\n",
      "\n",
      "# --------------------------------------------------------------\n",
      "# 2. LayerNorm (row\u2011wise)\n",
      "# --------------------------------------------------------------\n",
      "@triton.jit\n",
      "def layernorm_kernel(\n",
      "    X,               # [M, D]\n",
      "    Gamma, Beta,     # [D]\n",
      "    Out,\n",
      "    stride_xm, stride_xd,\n",
      "    stride_outm, stride_outd,\n",
      "    M, D,\n",
      "    EPS: tl.constexpr,\n",
      "    BLOCK_D: tl.constexpr,\n",
      "    num_warps: tl.constexpr\n",
      "):\n",
      "    pid = tl.program_id(0)\n",
      "\n",
      "    offs_m = pid\n",
      "    mask_m = offs_m < M\n",
      "\n",
      "    # load a whole row (or a tiled row if D > BLOCK_D)\n",
      "    offs_d = tl.arange(0, BLOCK_D)\n",
      "    mask_d = offs_d < D\n",
      "\n",
      "    x_ptr = X + offs_m * stride_xm + offs_d * stride_xd\n",
      "    x = tl.load(x_ptr, mask=mask_m & mask_d, other=0.0)\n",
      "\n",
      "    # compute mean\n",
      "    mean = tl.sum(x, axis=0) / D\n",
      "\n",
      "    # compute variance\n",
      "    var = tl.sum((x - mean) * (x - mean), axis=0) / D\n",
      "\n",
      "    # normalise\n",
      "    x_hat = (x - mean) / tl.sqrt(var + EPS)\n",
      "\n",
      "    # apply affine parameters\n",
      "    y = x_hat * Gamma[offs_d] + Beta[offs_d]\n",
      "\n",
      "    # store\n",
      "    out_ptr = Out + offs_m * stride_outm + offs_d * stride_outd\n",
      "    tl.store(out_ptr, y, mask=mask_m & mask_d)\n",
      "\n",
      "\n",
      "# --------------------------------------------------------------\n",
      "# 3. Row\u2011wise Softmax\n",
      "# --------------------------------------------------------------\n",
      "@triton.jit\n",
      "def softmax_kernel(\n",
      "    X,               # [M, N]\n",
      "    Out,\n",
      "    stride_xm, stride_xn,\n",
      "    stride_outm, stride_outn,\n",
      "    M, N,\n",
      "    BLOCK_N: tl.constexpr,\n",
      "    num_warps: tl.constexpr\n",
      "):\n",
      "    pid = tl.program_id(0)\n",
      "    offs_m = pid\n",
      "    mask_m = offs_m < M\n",
      "\n",
      "    offs_n = tl.arange(0, BLOCK_N)\n",
      "    mask_n = offs_n < N\n",
      "\n",
      "    x_ptr = X + offs_m * stride_xm + offs_n * stride_xn\n",
      "    x = tl.load(x_ptr, mask=mask_m & mask_n, other=-INF)\n",
      "\n",
      "    # max for numerical stability\n",
      "    row_max = tl.max(x, axis=0)\n",
      "    x_shift = x - row_max\n",
      "\n",
      "    exp_x = tl.exp(x_shift)\n",
      "    sum_exp = tl.sum(exp_x, axis=0)\n",
      "\n",
      "    y = exp_x / sum_exp\n",
      "\n",
      "    out_ptr = Out + offs_m * stride_outm + offs_n * stride_outn\n",
      "    tl.store(out_ptr, y, mask=mask_m & mask_n)\n",
      "\n",
      "\n",
      "# --------------------------------------------------------------\n",
      "# 4. Attention (single head, batched)\n",
      "# --------------------------------------------------------------\n",
      "@triton.jit\n",
      "def attention_kernel(\n",
      "    Q, K, V,               # shapes: [B, H, S, D_h]\n",
      "    Out,                   # shape: [B, H, S, D_h]\n",
      "    scale,                 # scalar float\n",
      "    causal: tl.constexpr,  # 0 or 1\n",
      "    padding_mask_ptr, tl.constexpr,  # optional pointer or nullptr\n",
      "    stride_qb, stride_qh, stride_qs, stride_qd,\n",
      "    stride_kb, stride_kh, stride_ks, stride_kd,\n",
      "    stride_vb, stride_vh, stride_vs, stride_vd,\n",
      "    stride_ob, stride_oh, stride_os, stride_od,\n",
      "    B, H, S, D_h,\n",
      "    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,\n",
      "    num_warps: tl.constexpr\n",
      "):\n",
      "    # program ids identify a tile of the (S, S) score matrix for a given (b, h)\n",
      "    pid_b = tl.program_id(0)\n",
      "    pid_h = tl.program_id(1)\n",
      "    pid_m = tl.program_id(2)   # row tile index\n",
      "    pid_n = tl.program_id(3)   # col tile index\n",
      "\n",
      "    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n",
      "    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n",
      "\n",
      "    mask_m = offs_m < S\n",
      "    mask_n = offs_n < S\n",
      "\n",
      "    # Load Q and K tiles\n",
      "    # Q: (S, D_h), K: (S, D_h) -> we need K^T : (D_h, S)\n",
      "    q_ptr = Q + pid_b * stride_qb + pid_h * stride_qh + offs_m * stride_qs + tl.arange(0, BLOCK_K) * stride_qd\n",
      "    k_ptr = K + pid_b * stride_kb + pid_h * stride_kh + tl.arange(0, BLOCK_K) * stride_ks + offs_n * stride_kd\n",
      "\n",
      "    q = tl.load(q_ptr, mask=mask_m[:, None] & (tl.arange(0, BLOCK_K)[None, :] < D_h), other=0.0)\n",
      "    k = tl.load(k_ptr, mask=(tl.arange(0, BLOCK_K)[:, None] < D_h) & mask_n[None, :], other=0.0)\n",
      "\n",
      "    # Compute raw scores (Q @ K^T)\n",
      "    scores = tl.dot(q, tl.trans(k)) * scale   # shape (BLOCK_M, BLOCK_N)\n",
      "\n",
      "    # Causal mask\n",
      "    if causal:\n",
      "        # create a triangular mask: allow only j <= i\n",
      "        row_idx = offs_m[:, None]\n",
      "        col_idx = offs_n[None, :]\n",
      "        causal_mask = row_idx < col_idx\n",
      "        scores = tl.where(causal_mask, -INF, scores)\n",
      "\n",
      "    # Padding mask (if provided)\n",
      "    if padding_mask_ptr != 0:\n",
      "        # padding_mask shape: [B, S] (bool, 1 = padded)\n",
      "        pad_ptr = padding_mask_ptr + pid_b * S\n",
      "        pad_row = tl.load(pad_ptr + offs_m, mask=mask_m, other=0)\n",
      "        # broadcast to columns\n",
      "        pad_mask = pad_row[:, None] & tl.ones([1, BLOCK_N], dtype=tl.int1)\n",
      "        scores = tl.where(pad_mask, -INF, scores)\n",
      "\n",
      "    # Softmax over columns (dim=-1)\n",
      "    row_max = tl.max(scores, axis=1)[:, None]\n",
      "    scores = tl.exp(scores - row_max)\n",
      "    row_sum = tl.sum(scores, axis=1)[:, None]\n",
      "    attn = scores / row_sum\n",
      "\n",
      "    # Multiply with V\n",
      "    v_ptr = V + pid_b * stride_vb + pid_h * stride_vh + offs_n * stride_vs + tl.arange(0, BLOCK_K) * stride_vd\n",
      "    v = tl.load(v_ptr, mask=(tl.arange(0, BLOCK_K)[:, None] < D_h) & mask_n[None, :], other=0.0)\n",
      "\n",
      "    out_tile = tl.dot(attn, v)   # (BLOCK_M, D_h)\n",
      "\n",
      "    # write output\n",
      "    out_ptr = Out + pid_b * stride_ob + pid_h * stride_oh + offs_m * stride_os + tl.arange(0, BLOCK_K) * stride_od\n",
      "    tl.store(out_ptr, out_tile, mask=mask_m[:, None] & (tl.arange(0, BLOCK_K)[None, :] < D_h))\n",
      "\n",
      "\n",
      "# --------------------------------------------------------------\n",
      "# 5. Wrapper that mimics TransformerEncoderLayer.forward\n",
      "# --------------------------------------------------------------\n",
      "def triton_kernel_wrapper(x, padding=None,\n",
      "                          dimension=4, n_heads=4, hidden=4,\n",
      "                          dropout=0.5, causal=False):\n",
      "    \"\"\"\n",
      "    x: Tensor of shape [B, S, D] (float32)\n",
      "    padding: optional bool tensor of shape [B, S] where True indicates a padded position\n",
      "    The function creates random weights for all linear layers and runs the full\n",
      "    Transformer encoder layer using Triton kernels.\n",
      "    \"\"\"\n",
      "    device = x.device\n",
      "    dtype = x.dtype\n",
      "    B, S, D = x.shape\n",
      "    assert D == dimension, \"input dimension must match the provided dimension\"\n",
      "\n",
      "    head_dim = D // n_heads\n",
      "    assert head_dim * n_heads == D, \"dimension must be divisible by n_heads\"\n",
      "\n",
      "    # ------------------------------------------------------------------\n",
      "    # Random weights (same dtype/device as input)\n",
      "    # ------------------------------------------------------------------\n",
      "    wq = torch.randn(D, D, device=device, dtype=dtype)\n",
      "    wk = torch.randn(D, D, device=device, dtype=dtype)\n",
      "    wv = torch.randn(D, D, device=device, dtype=dtype)\n",
      "    wo = torch.randn(D, D, device=device, dtype=dtype)   # output projection after concat\n",
      "\n",
      "    w1 = torch.randn(D, hidden, device=device, dtype=dtype)   # feed\u2011forward first linear\n",
      "    w2 = torch.randn(hidden, D, device=device, dtype=dtype)  # feed\u2011forward second linear\n",
      "\n",
      "    gamma1 = torch.ones(D, device=device, dtype=dtype)   # LayerNorm after attention\n",
      "    beta1  = torch.zeros(D, device=device, dtype=dtype)\n",
      "\n",
      "    gamma2 = torch.ones(D, device=device, dtype=dtype)   # LayerNorm after feed\u2011forward\n",
      "    beta2  = torch.zeros(D, device=device, dtype=dtype)\n",
      "\n",
      "    # ------------------------------------------------------------------\n",
      "    # Helper to launch GEMM (A @ B)\n",
      "    # ------------------------------------------------------------------\n",
      "    def linear(input_tensor, weight):\n",
      "        # input: [*, in_dim]   weight: [in_dim, out_dim]\n",
      "        out_shape = list(input_tensor.shape[:-1]) + [weight.shape[1]]\n",
      "        out = torch.empty(out_shape, device=device, dtype=dtype)\n",
      "\n",
      "        M = input_tensor.numel() // input_tensor.shape[-1]          # product of all leading dims\n",
      "        K = input_tensor.shape[-1]\n",
      "        N = weight.shape[1]\n",
      "\n",
      "        BLOCK_M = 64\n",
      "        BLOCK_N = 64\n",
      "        BLOCK_K = 32\n",
      "\n",
      "        grid = (triton.cdiv(M, BLOCK_M), triton.cdiv(N, BLOCK_N))\n",
      "\n",
      "        # strides\n",
      "        stride_am = K\n",
      "        stride_ak = 1\n",
      "        stride_bk = 1\n",
      "        stride_bn = N\n",
      "        stride_cm = N\n",
      "        stride_cn = 1\n",
      "\n",
      "        gemm_kernel[grid](\n",
      "            input_tensor, weight, out,\n",
      "            stride_am, stride_ak,\n",
      "            stride_bk, stride_bn,\n",
      "            stride_cm, stride_cn,\n",
      "            M, N, K,\n",
      "            BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N, BLOCK_K=BLOCK_K,\n",
      "            num_warps=4\n",
      "        )\n",
      "        return out\n",
      "\n",
      "    # ------------------------------------------------------------------\n",
      "    # 1. Linear projections Q, K, V\n",
      "    # ------------------------------------------------------------------\n",
      "    Q = linear(x, wq)   # [B, S, D]\n",
      "    K = linear(x, wk)\n",
      "    V = linear(x, wv)\n",
      "\n",
      "    # ------------------------------------------------------------------\n",
      "    # 2. Split heads\n",
      "    # ------------------------------------------------------------------\n",
      "    # reshape to [B, S, n_heads, head_dim] then transpose to [B, n_heads, S, head_dim]\n",
      "    def split_heads(t):\n",
      "        t = t.view(B, S, n_heads, head_dim)\n",
      "        return t.permute(0, 2, 1, 3).contiguous()\n",
      "    Qh = split_heads(Q)\n",
      "    Kh = split_heads(K)\n",
      "    Vh = split_heads(V)\n",
      "\n",
      "    # ------------------------------------------------------------------\n",
      "    # 3. Attention per head (batched)\n",
      "    # ------------------------------------------------------------------\n",
      "    # allocate output tensor for attention result\n",
      "    attn_out = torch.empty_like(Qh)   # [B, n_heads, S, head_dim]\n",
      "\n",
      "    # launch attention kernel\n",
      "    # grid dimensions: B, n_heads, ceil(S/BLOCK_M), ceil(S/BLOCK_N)\n",
      "    BLOCK_M = 32\n",
      "    BLOCK_N = 32\n",
      "    BLOCK_K = head_dim   # since head_dim is usually small (e.g., 1)\n",
      "\n",
      "    grid = (\n",
      "        B,\n",
      "        n_heads,\n",
      "        triton.cdiv(S, BLOCK_M),\n",
      "        triton.cdiv(S, BLOCK_N)\n",
      "    )\n",
      "\n",
      "    # strides for the 4\u2011D tensors (contiguous layout)\n",
      "    stride_qb, stride_qh, stride_qs, stride_qd = Qh.stride()\n",
      "    stride_kb, stride_kh, stride_ks, stride_kd = Kh.stride()\n",
      "    stride_vb, stride_vh, stride_vs, stride_vd = Vh.stride()\n",
      "    stride_ob, stride_oh, stride_os, stride_od = attn_out.stride()\n",
      "\n",
      "    # optional padding mask pointer\n",
      "    padding_ptr = 0\n",
      "    if padding is not None:\n",
      "        padding_ptr = padding.contiguous().view(-1).data_ptr()\n",
      "\n",
      "    attention_kernel[grid](\n",
      "        Qh, Kh, Vh, attn_out,\n",
      "        1.0 / (head_dim ** 0.5),\n",
      "        causal,\n",
      "        padding_ptr,\n",
      "        stride_qb, stride_qh, stride_qs, stride_qd,\n",
      "        stride_kb, stride_kh, stride_ks, stride_kd,\n",
      "        stride_vb, stride_vh, stride_vs, stride_vd,\n",
      "        stride_ob, stride_oh, stride_os, stride_od,\n",
      "        B, n_heads, S, head_dim,\n",
      "        BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N, BLOCK_K=BLOCK_K,\n",
      "        num_warps=4\n",
      "    )\n",
      "\n",
      "    # ------------------------------------------------------------------\n",
      "    # 4. Concatenate heads and final linear projection\n",
      "    # ------------------------------------------------------------------\n",
      "    def combine_heads(t):\n",
      "        # t: [B, n_heads, S, head_dim] -> [B, S, D]\n",
      "        t = t.permute(0, 2, 1, 3).contiguous()\n",
      "        return t.view(B, S, D)\n",
      "    attn_concat = combine_heads(attn_out)          # [B, S, D]\n",
      "    attn_proj = linear(attn_concat, wo)            # [B, S, D]\n",
      "\n",
      "    # ------------------------------------------------------------------\n",
      "    # 5. First Residual + LayerNorm\n",
      "    # ------------------------------------------------------------------\n",
      "    res1 = x + attn_proj\n",
      "\n",
      "    out1 = torch.empty_like(res1)\n",
      "    M = B * S\n",
      "    BLOCK_D = 128\n",
      "    grid_ln1 = (triton.cdiv(M, 1),)   # one program per row\n",
      "    layernorm_kernel[grid_ln1](\n",
      "        res1, gamma1, beta1, out1,\n",
      "        stride_xm=res1.stride(0), stride_xd=res1.stride(2),\n",
      "        stride_outm=out1.stride(0), stride_outd=out1.stride(2),\n",
      "        M, D,\n",
      "        EPS=1e-6,\n",
      "        BLOCK_D=BLOCK_D,\n",
      "        num_warps=2\n",
      "    )\n",
      "\n",
      "    # ------------------------------------------------------------------\n",
      "    # 6. Feed\u2011forward network\n",
      "    # ------------------------------------------------------------------\n",
      "    ff1 = linear(out1, w1)               # [B, S, hidden]\n",
      "    # ReLU\n",
      "    ff1_relu = torch.maximum(ff1, torch.tensor(0.0, device=device, dtype=dtype))\n",
      "    ff2 = linear(ff1_relu, w2)           # [B, S, D]\n",
      "\n",
      "    # ------------------------------------------------------------------\n",
      "    # 7. Second Residual + LayerNorm\n",
      "    # ------------------------------------------------------------------\n",
      "    res2 = out1 + ff2\n",
      "    out2 = torch.empty_like(res2)\n",
      "    grid_ln2 = (triton.cdiv(M, 1),)\n",
      "    layernorm_kernel[grid_ln2](\n",
      "        res2, gamma2, beta2, out2,\n",
      "        stride_xm=res2.stride(0), stride_xd=res2.stride(2),\n",
      "        stride_outm=out2.stride(0), stride_outd=out2.stride(2),\n",
      "        M, D,\n",
      "        EPS=1e-6,\n",
      "        BLOCK_D=BLOCK_D,\n",
      "        num_warps=2\n",
      "    )\n",
      "\n",
      "    return out2\n",
      "</triton>...\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "source": [
    "def load_model_and_tokenizer(\n",
    "    model_name: str = MODEL_NAME,\n",
    "    use_4bit: bool = True,\n",
    "    use_flash_attention: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Load the Trinity-Mini model with quantization and LoRA for efficient finetuning.\n",
    "    \n",
    "    Args:\n",
    "        model_name: HuggingFace model name\n",
    "        use_4bit: Whether to use 4-bit quantization (recommended for 26B model)\n",
    "        use_flash_attention: Whether to use Flash Attention 2\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (model, tokenizer)\n",
    "    \"\"\"\n",
    "    print(f\"Loading tokenizer for {model_name}...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    \n",
    "    # Set padding token if not present (required for Trinity-Mini)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "        print(f\"Set pad_token to eos_token: {tokenizer.pad_token}\")\n",
    "\n",
    "    # Load and modify config (required for AFMoE models like Trinity-Mini)\n",
    "    from transformers import AutoConfig\n",
    "    print(\"Loading model config...\")\n",
    "    config = AutoConfig.from_pretrained(model_name)\n",
    "    config.pad_token_id = tokenizer.pad_token_id\n",
    "    print(f\"Set config.pad_token_id = {config.pad_token_id}\")\n",
    "\n",
    "    # Configure quantization\n",
    "    if use_4bit:\n",
    "        print(\"Configuring 4-bit quantization...\")\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_use_double_quant=True,  # Nested quantization for more memory savings\n",
    "            bnb_4bit_quant_type=\"nf4\",  # Normal Float 4 quantization\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16,  # Compute in bfloat16\n",
    "        )\n",
    "    else:\n",
    "        bnb_config = None\n",
    "    \n",
    "    # Model loading kwargs\n",
    "    model_kwargs = {\n",
    "        \"config\": config,  # Pass modified config with pad_token_id\n",
    "        \"torch_dtype\": torch.bfloat16,\n",
    "        \"device_map\": \"auto\",\n",
    "    }\n",
    "    \n",
    "    if bnb_config:\n",
    "        model_kwargs[\"quantization_config\"] = bnb_config\n",
    "    \n",
    "    if use_flash_attention:\n",
    "        try:\n",
    "            model_kwargs[\"attn_implementation\"] = \"flash_attention_2\"\n",
    "            print(\"Using Flash Attention 2\")\n",
    "        except Exception:\n",
    "            print(\"Flash Attention 2 not available, using default attention\")\n",
    "    \n",
    "    print(f\"Loading model {model_name}...\")\n",
    "    print(\"This may take a few minutes for a 26B parameter model...\")\n",
    "    \n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        **model_kwargs,\n",
    "        \n",
    "    )\n",
    "    \n",
    "    # Prepare model for k-bit training (required for QLoRA)\n",
    "    if use_4bit:\n",
    "        model = prepare_model_for_kbit_training(model)\n",
    "    \n",
    "    print(f\"Model loaded successfully!\")\n",
    "    print(f\"  Model type: {type(model).__name__}\")\n",
    "    print(f\"  Device map: {model.hf_device_map if hasattr(model, 'hf_device_map') else 'N/A'}\")\n",
    "    \n",
    "    return model, tokenizer\n"
   ],
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "source": [
    "%uv pip install --upgrade transformers\n"
   ],
   "execution_count": 11,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[2mUsing Python 3.12.6 environment at: /usr/local\u001b[0m\r\n",
      "\u001b[37m\u280b\u001b[0m \u001b[2mResolving dependencies...                                                     \u001b[0m\r\u001b[2K\u001b[37m\u280b\u001b[0m \u001b[2mResolving dependencies...                                                     \u001b[0m\r\u001b[2K\u001b[37m\u2819\u001b[0m \u001b[2mResolving dependencies...                                                     \u001b[0m\r\u001b[2K\u001b[37m\u2819\u001b[0m \u001b[2mtransformers==5.0.0                                                           \u001b[0m\r\u001b[2K\u001b[37m\u2819\u001b[0m \u001b[2mfilelock==3.20.3                                                              \u001b[0m\r\u001b[2K\u001b[37m\u2819\u001b[0m \u001b[2mhuggingface-hub==1.3.7                                                        \u001b[0m\r\u001b[2K\u001b[37m\u2819\u001b[0m \u001b[2mnumpy==2.4.2                                                                  \u001b[0m\r\u001b[2K\u001b[37m\u2819\u001b[0m \u001b[2mpackaging==26.0                                                               \u001b[0m\r\u001b[2K\u001b[37m\u2819\u001b[0m \u001b[2mpyyaml==6.0.3                                                                 \u001b[0m\r\u001b[2K\u001b[37m\u2819\u001b[0m \u001b[2mregex==2026.1.15                                                              \u001b[0m\r\u001b[2K\u001b[37m\u2819\u001b[0m \u001b[2mtokenizers==0.22.2                                                            \u001b[0m\r\u001b[2K\u001b[37m\u2819\u001b[0m \u001b[2mtyper-slim==0.21.1                                                            \u001b[0m\r\u001b[2K\u001b[37m\u2819\u001b[0m \u001b[2msafetensors==0.7.0                                                            \u001b[0m\r\u001b[2K\u001b[37m\u2819\u001b[0m \u001b[2mtqdm==4.67.3                                                                  \u001b[0m\r\u001b[2K\u001b[37m\u2819\u001b[0m \u001b[2mfsspec==2026.1.0                                                              \u001b[0m\r\u001b[2K\u001b[37m\u2819\u001b[0m \u001b[2mhf-xet==1.2.0                                                                 \u001b[0m\r\u001b[2K\u001b[37m\u2819\u001b[0m \u001b[2mhf-xet==1.2.0                                                                 \u001b[0m\r\u001b[2K\u001b[37m\u2819\u001b[0m \u001b[2mhttpx==0.28.1                                                                 \u001b[0m\r\u001b[2K\u001b[37m\u2819\u001b[0m \u001b[2mshellingham==1.5.4                                                            \u001b[0m\r\u001b[2K\u001b[37m\u2819\u001b[0m \u001b[2mtyping-extensions==4.15.0                                                     \u001b[0m\r\u001b[2K\u001b[37m\u2819\u001b[0m \u001b[2mclick==8.3.1                                                                  \u001b[0m\r\u001b[2K\u001b[37m\u2819\u001b[0m \u001b[2manyio==4.12.1                                                                 \u001b[0m\r\u001b[2K\u001b[2mResolved \u001b[1m22 packages\u001b[0m \u001b[2min 65ms\u001b[0m\u001b[0m\r\n",
      "\u001b[2mAudited \u001b[1m22 packages\u001b[0m \u001b[2min 0.07ms\u001b[0m\u001b[0m\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "source": [
    "import transformers\n",
    "print(transformers.__version__)"
   ],
   "execution_count": 12,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "5.0.0\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "source": [
    "# Weights & Biases configuration\n",
    "WANDB_PROJECT = \"trinity-triton-sft\"\n",
    "WANDB_RUN_NAME = \"trinity-sft-run\"  # A\n",
    "%uv pip install wandb"
   ],
   "execution_count": 13,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[2mUsing Python 3.12.6 environment at: /usr/local\u001b[0m\r\n",
      "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 5ms\u001b[0m\u001b[0m\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "source": [
    "def create_lora_config(\n",
    "    r: int = LORA_R,\n",
    "    lora_alpha: int = LORA_ALPHA,\n",
    "    lora_dropout: float = LORA_DROPOUT,\n",
    "    target_modules: Optional[List[str]] = None,\n",
    ") -> LoraConfig:\n",
    "    \"\"\"\n",
    "    Create LoRA configuration for Trinity-Mini.\n",
    "    \n",
    "    Trinity-Mini uses an MoE architecture, so we target:\n",
    "    - Query, Key, Value projections in attention\n",
    "    - Gate and up/down projections in MLP\n",
    "    - Optionally the expert layers (if applicable)\n",
    "    \n",
    "    Args:\n",
    "        r: LoRA rank (higher = more parameters to train)\n",
    "        lora_alpha: LoRA alpha (scaling factor, typically 2*r)\n",
    "        lora_dropout: Dropout probability for LoRA layers\n",
    "        target_modules: List of module names to apply LoRA to\n",
    "        \n",
    "    Returns:\n",
    "        LoraConfig instance\n",
    "    \"\"\"\n",
    "    if target_modules is None:\n",
    "        # Default targets for typical LLM architectures\n",
    "        # This should work for most transformer models\n",
    "        target_modules = [\n",
    "            \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",  # Attention\n",
    "            \"gate_proj\", \"up_proj\", \"down_proj\",      # MLP\n",
    "        ]\n",
    "    \n",
    "    lora_config = LoraConfig(\n",
    "        r=r,\n",
    "        lora_alpha=lora_alpha,\n",
    "        lora_dropout=lora_dropout,\n",
    "        target_modules=target_modules,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "    \n",
    "    print(\"LoRA Configuration:\")\n",
    "    print(f\"  Rank (r):          {r}\")\n",
    "    print(f\"  Alpha:             {lora_alpha}\")\n",
    "    print(f\"  Dropout:           {lora_dropout}\")\n",
    "    print(f\"  Target modules:    {target_modules}\")\n",
    "    \n",
    "    return lora_config"
   ],
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "source": [
    "lora_config = create_lora_config()\n"
   ],
   "execution_count": 15,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "LoRA Configuration:\n",
      "  Rank (r):          64\n",
      "  Alpha:             128\n",
      "  Dropout:           0.05\n",
      "  Target modules:    ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "source": "import os\n# Set your W&B API key (get it from https://wandb.ai/authorize)\nos.environ[\"WANDB_API_KEY\"] = os.getenv(\"WANDB_API_KEY\", \"\")  # Set in environment or .env file",
   "execution_count": 16,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "source": [
    "def create_training_config(\n",
    "    output_dir: str = OUTPUT_DIR,\n",
    "    learning_rate: float = LEARNING_RATE,\n",
    "    batch_size: int = BATCH_SIZE,\n",
    "    gradient_accumulation_steps: int = GRADIENT_ACCUMULATION_STEPS,\n",
    "    num_epochs: int = NUM_EPOCHS,\n",
    "    max_seq_length: int = MAX_LENGTH,\n",
    "    warmup_ratio: float = WARMUP_RATIO,\n",
    ") -> SFTConfig:\n",
    "    \"\"\"\n",
    "    Create SFTConfig with optimized settings for Trinity-Mini finetuning.\n",
    "    \n",
    "    Returns:\n",
    "        SFTConfig instance\n",
    "    \"\"\"\n",
    "    config = SFTConfig(\n",
    "        output_dir=output_dir,\n",
    "        \n",
    "        # Training parameters\n",
    "        num_train_epochs=num_epochs,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        \n",
    "        # Learning rate\n",
    "        learning_rate=learning_rate,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        warmup_ratio=warmup_ratio,\n",
    "        \n",
    "        # Optimizer\n",
    "        optim=\"adamw_torch_fused\" if torch.cuda.is_available() else \"adamw_torch\",\n",
    "        weight_decay=0.01,\n",
    "        max_grad_norm=1.0,\n",
    "        \n",
    "        # Sequence settings\n",
    "        max_length=MAX_LENGTH,\n",
    "        packing=False,  # Don't pack examples (each kernel trace is independent)\n",
    "        \n",
    "        # Memory optimization\n",
    "        gradient_checkpointing=True,\n",
    "        gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "        \n",
    "        # Precision\n",
    "        bf16=torch.cuda.is_bf16_supported() if torch.cuda.is_available() else False,\n",
    "        fp16=not (torch.cuda.is_bf16_supported() if torch.cuda.is_available() else True),\n",
    "        \n",
    "        # Logging (Weights & Biases)\n",
    "        logging_steps=10,\n",
    "        logging_first_step=True,\n",
    "        report_to=[\"wandb\"],\n",
    "        run_name=WANDB_RUN_NAME,  # W&B run name\n",
    "        \n",
    "        # Evaluation\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=20,  # Changed to match save frequency\n",
    "\n",
    "        # CHECKPOINT SAVING - FIXED\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=20,  # Save every 20 iterations (changed from 100)\n",
    "        save_total_limit=10,  # Increased to keep more checkpoints\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        \n",
    "        # Misc\n",
    "        seed=42,\n",
    "        dataloader_num_workers=4,\n",
    "        remove_unused_columns=True,\n",
    "    )\n",
    "    \n",
    "    print(\"\\nTraining Configuration:\")\n",
    "    print(f\"  Output directory:     {output_dir}\")\n",
    "    print(f\"  Epochs:               {num_epochs}\")\n",
    "    print(f\"  Batch size:           {batch_size}\")\n",
    "    print(f\"  Gradient accum:       {gradient_accumulation_steps}\")\n",
    "    print(f\"  Effective batch:      {batch_size * gradient_accumulation_steps}\")\n",
    "    print(f\"  Learning rate:        {learning_rate}\")\n",
    "    print(f\"  Max sequence length:  {max_seq_length}\")\n",
    "    print(f\"  Gradient checkpoint:  True\")\n",
    "    print(f\"  Logging:              Weights & Biases\")\n",
    "    \n",
    "    return config\n",
    "\n",
    "\n",
    "training_config = create_training_config()"
   ],
   "execution_count": 17,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "warmup_ratio is deprecated and will be removed in v5.2. Use `warmup_steps` instead.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Training Configuration:\n",
      "  Output directory:     /mnt/models/trinity-triton-sft\n",
      "  Epochs:               3\n",
      "  Batch size:           1\n",
      "  Gradient accum:       8\n",
      "  Effective batch:      8\n",
      "  Learning rate:        0.0002\n",
      "  Max sequence length:  4096\n",
      "  Gradient checkpoint:  True\n",
      "  Logging:              Weights & Biases\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "source": [
    "def train(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    dataset: DatasetDict,\n",
    "    training_config: SFTConfig,\n",
    "    lora_config: LoraConfig,\n",
    "    wandb_project: str = WANDB_PROJECT,\n",
    "):\n",
    "    \"\"\"\n",
    "    Initialize SFTTrainer and start training with W&B logging.\n",
    "    \"\"\"\n",
    "    print(\"\\\\n\" + \"=\" * 60)\n",
    "    print(\"INITIALIZING SFT TRAINER\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Initialize Weights & Biases\n",
    "    import wandb\n",
    "    wandb.init(\n",
    "        project=wandb_project,\n",
    "        name=training_config.run_name,\n",
    "        config={\n",
    "            \"model\": MODEL_NAME,\n",
    "            \"lora_r\": lora_config.r,\n",
    "            \"lora_alpha\": lora_config.lora_alpha,\n",
    "            \"learning_rate\": training_config.learning_rate,\n",
    "            \"batch_size\": training_config.per_device_train_batch_size,\n",
    "            \"gradient_accumulation\": training_config.gradient_accumulation_steps,\n",
    "            \"epochs\": training_config.num_train_epochs,\n",
    "            \"max_length\": training_config.max_length,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        train_dataset=dataset[\"train\"],\n",
    "        eval_dataset=dataset[\"test\"],\n",
    "        args=training_config,\n",
    "        peft_config=lora_config,\n",
    "        processing_class=tokenizer,  # Explicitly pass tokenizer for proper saving\n",
    "    )\n",
    "\n",
    "    # Print trainable parameters\n",
    "    trainable_params = sum(p.numel() for p in trainer.model.parameters() if p.requires_grad)\n",
    "    total_params = sum(p.numel() for p in trainer.model.parameters())\n",
    "    print(f\"\\\\nTrainable parameters: {trainable_params:,}\")\n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    print(f\"Trainable %: {100 * trainable_params / total_params:.2f}%\")\n",
    "\n",
    "    print(\"\\\\n\" + \"=\" * 60)\n",
    "    print(\"STARTING TRAINING\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Train! Checkpoints will be saved automatically every 20 steps\n",
    "    trainer.train()\n",
    "\n",
    "    # Save the final model explicitly (in addition to checkpoints)\n",
    "    print(\"\\\\nSaving final model...\")\n",
    "    final_output_dir = os.path.join(training_config.output_dir, \"final_model\")\n",
    "    trainer.save_model(final_output_dir)\n",
    "    \n",
    "    # Also save just the adapter weights for easier loading\n",
    "    trainer.model.save_pretrained(final_output_dir)\n",
    "\n",
    "    # Save tokenizer\n",
    "    tokenizer.save_pretrained(final_output_dir)\n",
    "    tokenizer.save_pretrained(training_config.output_dir)\n",
    "\n",
    "    print(f\"\\\\nTraining complete!\")\n",
    "    print(f\"Checkpoints saved to: {training_config.output_dir}/checkpoint-*\")\n",
    "    print(f\"Final model saved to: {final_output_dir}\")\n",
    "\n",
    "    # Finish W&B run\n",
    "    wandb.finish()\n",
    "\n",
    "    return trainer\n",
    "\n"
   ],
   "execution_count": 18,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main training pipeline.\n",
    "    \n",
    "    1. Load and filter reasoning traces\n",
    "    2. Prepare dataset in SFT format\n",
    "    3. Load Trinity-Mini with quantization\n",
    "    4. Apply LoRA for efficient finetuning\n",
    "    5. Train with SFTTrainer\n",
    "    6. Save the finetuned model\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"TRINITY-MINI FINETUNING PIPELINE\")\n",
    "    print(\"Finetuning on PyTorch \u2192 Triton reasoning traces\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Step 1: Load traces (includes ALL traces: correct + failed)\n",
    "    print(\"\\n[Step 1/5] Loading traces...\")\n",
    "    try:\n",
    "        traces = load_and_filter_traces(TRACES_FILE)  # No filtering - include all\n",
    "    except FileNotFoundError:\n",
    "        print(\"ERROR: No traces file found!\")\n",
    "        print(\"Please run the orchestrator first to generate traces:\")\n",
    "        print(\"  cd .. && python orchestrator.py\")\n",
    "        return\n",
    "\n",
    "    if len(traces) < 10:\n",
    "        print(f\"WARNING: Only {len(traces)} traces available.\")\n",
    "        print(\"Consider generating more traces for better training.\")\n",
    "    \n",
    "    # Step 2: Prepare dataset\n",
    "    print(\"\\n[Step 2/5] Preparing dataset...\")\n",
    "    dataset = prepare_dataset(traces)\n",
    "    \n",
    "    # Step 3: Load model\n",
    "    print(\"\\n[Step 3/5] Loading Trinity-Mini model...\")\n",
    "    model, tokenizer = load_model_and_tokenizer()\n",
    "    \n",
    "    # Step 4: Configure LoRA\n",
    "    print(\"\\n[Step 4/5] Configuring LoRA...\")\n",
    "    lora_config = create_lora_config()\n",
    "    \n",
    "    # Step 5: Train\n",
    "    print(\"\\n[Step 5/5] Starting training...\")\n",
    "    training_config = create_training_config()\n",
    "    trainer = train(model, tokenizer, dataset, training_config, lora_config)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"TRAINING COMPLETE!\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Model saved to: {OUTPUT_DIR}\")\n",
    "    print(\"\\nTo use the finetuned model:\")\n",
    "    print(f\"\"\"\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\"{OUTPUT_DIR}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"{OUTPUT_DIR}\")\n",
    "\"\"\")\n",
    "    \n",
    "    return trainer"
   ],
   "execution_count": 19,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "execution_count": 20,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "/usr/local/lib/python3.12/site-packages/transformers/modeling_rope_utils.py:927: FutureWarning: `rope_config_validation` is deprecated and has been removed. Its functionality has been moved to RotaryEmbeddingConfigMixin.validate_rope method. PreTrainedConfig inherits this class, so please call self.validate_rope() instead. Also, make sure to use the new rope_parameters syntax. You can call self.standardize_rope_params() in the meantime.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "============================================================\n",
      "TRINITY-MINI FINETUNING PIPELINE\n",
      "Finetuning on PyTorch \u2192 Triton reasoning traces\n",
      "============================================================\n",
      "\n",
      "[Step 1/5] Loading traces...\n",
      "Loaded 158 total traces\n",
      "\n",
      "============================================================\n",
      "DATASET LOADING SUMMARY\n",
      "============================================================\n",
      "Total traces loaded:       158\n",
      "  \u2713 Correct kernels:       133 (84.2%)\n",
      "  \u2717 Failed kernels:        25 (15.8%)\n",
      "------------------------------------------------------------\n",
      "Final training set:        158 traces\n",
      "  Training mix:            84.2% correct, 15.8% failed\n",
      "============================================================\n",
      "\n",
      "\n",
      "[Step 2/5] Preparing dataset...\n",
      "\n",
      "[Step 3/5] Loading Trinity-Mini model...\n",
      "Loading tokenizer for arcee-ai/Trinity-Mini...\n",
      "Loading model config...\n",
      "Set config.pad_token_id = 12\n",
      "Configuring 4-bit quantization...\n",
      "Loading model arcee-ai/Trinity-Mini...\n",
      "This may take a few minutes for a 26B parameter model...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f77a9c4576f4347a31fd6a2de7ba802",
       "version_minor": 0.0,
       "version_major": 2.0
      },
      "text/plain": "Loading weights:   0%|          | 0/12031 [00:00<?, ?it/s]"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'min_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "warmup_ratio is deprecated and will be removed in v5.2. Use `warmup_steps` instead.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: [wandb.login()] Loaded credentials for https://api.wandb.ai from WANDB_API_KEY.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model loaded successfully!\n",
      "  Model type: AfmoeForCausalLM\n",
      "  Device map: N/A\n",
      "\n",
      "[Step 4/5] Configuring LoRA...\n",
      "LoRA Configuration:\n",
      "  Rank (r):          64\n",
      "  Alpha:             128\n",
      "  Dropout:           0.05\n",
      "  Target modules:    ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']\n",
      "\n",
      "[Step 5/5] Starting training...\n",
      "\n",
      "Training Configuration:\n",
      "  Output directory:     /mnt/models/trinity-triton-sft\n",
      "  Epochs:               3\n",
      "  Batch size:           1\n",
      "  Gradient accum:       8\n",
      "  Effective batch:      8\n",
      "  Learning rate:        0.0002\n",
      "  Max sequence length:  4096\n",
      "  Gradient checkpoint:  True\n",
      "  Logging:              Weights & Biases\n",
      "\\n============================================================\n",
      "INITIALIZING SFT TRAINER\n",
      "============================================================\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mppbhatt500\u001b[0m (\u001b[33mppbhatt500-verizon\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.24.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/root/wandb/run-20260204_060831-fh1jdjwn\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mtrinity-sft-run\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u2b50\ufe0f View project at \u001b[34m\u001b[4mhttps://wandb.ai/ppbhatt500-verizon/trinity-triton-sft\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ud83d\ude80 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ppbhatt500-verizon/trinity-triton-sft/runs/fh1jdjwn\u001b[0m\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45ab03d1542f4266b4aea31ac3e8e112",
       "version_minor": 0.0,
       "version_major": 2.0
      },
      "text/plain": "Tokenizing train dataset:   0%|          | 0/142 [00:00<?, ? examples/s]"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f48a6aada6b94b5c9cd6e6be7f9becd6",
       "version_minor": 0.0,
       "version_major": 2.0
      },
      "text/plain": "Truncating train dataset:   0%|          | 0/142 [00:00<?, ? examples/s]"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3b18d2ee2d14b659a4baab49d6df7e8",
       "version_minor": 0.0,
       "version_major": 2.0
      },
      "text/plain": "Tokenizing eval dataset:   0%|          | 0/16 [00:00<?, ? examples/s]"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bb99dac6edd4082bfee8d983d97be29",
       "version_minor": 0.0,
       "version_major": 2.0
      },
      "text/plain": "Truncating eval dataset:   0%|          | 0/16 [00:00<?, ? examples/s]"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Detected kernel version 4.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 3, 'bos_token_id': 0, 'pad_token_id': 12}.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\\nTrainable parameters: 2,333,999,104\n",
      "Total parameters: 15,806,117,632\n",
      "Trainable %: 14.77%\n",
      "\\n============================================================\n",
      "STARTING TRAINING\n",
      "============================================================\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": "\n    <div>\n      \n      <progress value='27' max='54' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [27/54 54:49 < 59:12, 0.01 it/s, Epoch 1.45/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>20</td>\n      <td>2.421357</td>\n      <td>0.810319</td>\n    </tr>\n  </tbody>\n</table><p>",
      "text/plain": "<IPython.core.display.HTML object>"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "source": [
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "source = Path(\"/mnt/models/trinity-triton-sft\")\n",
    "dest = Path(\"/mnt/arcee-vol/models/trinity-triton-sft\")\n",
    "\n",
    "dest.parent.mkdir(parents=True, exist_ok=True)\n",
    "shutil.copytree(source, dest, dirs_exist_ok=True)\n",
    "\n",
    "print(f\"Checkpoints copied: {list(dest.glob('checkpoint-*'))}\")"
   ],
   "execution_count": 30,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "source": [
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "source_cp40 = Path(\"/mnt/models/trinity-triton-sft/checkpoint-40\")\n",
    "dest_cp40 = Path(\"/mnt/arcee-vol/models/trinity-triton-sft/checkpoint-40\")\n",
    "\n",
    "if source_cp40.exists():\n",
    "    print(f\"Copying checkpoint-40 ({sum(f.stat().st_size for f in source_cp40.rglob('*') if f.is_file()) / 1e9:.2f} GB)...\")\n",
    "    shutil.copytree(source_cp40, dest_cp40, dirs_exist_ok=True)\n",
    "    print(\"\u2713 Done\")\n",
    "else:\n",
    "    print(\"\u274c checkpoint-40 not found in original location\")"
   ],
   "execution_count": 31,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "source": [
    "def inference(\n",
    "    pytorch_code: str,\n",
    "    model_path: str = OUTPUT_DIR,\n",
    "    max_new_tokens: int = 2048,\n",
    "):\n",
    "    \"\"\"\n",
    "    Run inference with the finetuned model.\n",
    "    \n",
    "    Args:\n",
    "        pytorch_code: PyTorch code to convert to Triton\n",
    "        model_path: Path to the finetuned model\n",
    "        max_new_tokens: Maximum tokens to generate\n",
    "        \n",
    "    Returns:\n",
    "        Generated Triton kernel with reasoning\n",
    "    \"\"\"\n",
    "    from peft import AutoPeftModelForCausalLM\n",
    "    \n",
    "    print(\"Loading finetuned model...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "        model_path,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "    \n",
    "    # Format input as conversation\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"\"\"Convert the following PyTorch code to an optimized Triton kernel:\n",
    "\n",
    "```python\n",
    "{pytorch_code}\n",
    "```\n",
    "\n",
    "Generate a complete Triton implementation that produces the same output as the PyTorch code.\"\"\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Apply chat template\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "    )\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # Generate\n",
    "    print(\"Generating Triton kernel...\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.95,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    # Decode\n",
    "    response = tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
    "    \n",
    "    return response"
   ],
   "execution_count": 21,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "source": [],
   "execution_count": 23,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "source": [],
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}